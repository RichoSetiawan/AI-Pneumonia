{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10367803,"sourceType":"datasetVersion","datasetId":6421619},{"sourceId":10409654,"sourceType":"datasetVersion","datasetId":6450952}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras import layers, models, optimizers\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, RocCurveDisplay\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n# Base directory dataset (pastikan struktur direktori sesuai dataset Pneumonia)\nbase_dir = \"/kaggle/input/pneumonia\"\ntrain_dir = os.path.join(base_dir, \"Pneumonia/train\")\nval_dir = os.path.join(base_dir, \"Pneumonia/val\")\ntest_dir = os.path.join(base_dir, \"Pneumonia/test\")\n\n# Verifikasi direktori\nfor d in [train_dir, val_dir, test_dir]:\n    if not os.path.exists(d):\n        raise FileNotFoundError(f\"Directory not found: {d}\")\n\n# ============================\n# Data Preparation & Augmentasi\n# ============================\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary'\n)\nval_gen = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary'\n)\ntest_gen = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary',\n    shuffle=False\n)\n\n# ============================\n# Class Weights (imbalance)\n# ============================\nclasses = np.unique(train_gen.classes)\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_gen.classes)\nclass_weights = dict(enumerate(class_weights))\n\n# ============================\n# Model: EfficientNetB0\n# ============================\nbase_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nbase_model.trainable = False  # Bekukan bobot pretrained\n\nmodel = models.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')  # Keluaran sigmoid untuk klasifikasi biner\n])\n\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# ============================\n# Callbacks: EarlyStopping & ReduceLROnPlateau\n# ============================\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n]\n\n# ============================\n# Training awal dengan Frozen Base\n# ============================\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    class_weight=class_weights,\n    callbacks=callbacks\n)\n\n# ============================\n# Fine-Tuning: buka kembali base_model\n# ============================\nbase_model.trainable = True\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=0.0001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\nhistory_fine = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    class_weight=class_weights,\n    callbacks=callbacks\n)\n\n# ============================\n# Evaluasi pada Test Set\n# ============================\ntest_loss, test_acc = model.evaluate(test_gen, verbose=0)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n\ny_true = test_gen.classes\ny_pred_probs = model.predict(test_gen)\ny_pred = (y_pred_probs > 0.5).astype(int).reshape(-1)\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification Report\ntarget_names = list(test_gen.class_indices.keys())\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=target_names))\n\n# ROC AUC Score\nroc_auc = roc_auc_score(y_true, y_pred_probs)\nprint(f\"AUROC: {roc_auc:.2f}\")\n\n# Plot ROC Curve\nRocCurveDisplay.from_predictions(y_true, y_pred_probs)\nplt.title(\"ROC Curve\")\nplt.show()\n\n# Plot Training History\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(history.history['accuracy'], label='Train Acc (phase 1)')\nplt.plot(history.history['val_accuracy'], label='Val Acc (phase 1)')\nplt.plot(history_fine.history['accuracy'], label='Train Acc (phase 2)')\nplt.plot(history_fine.history['val_accuracy'], label='Val Acc (phase 2)')\nplt.title(\"Akurasi Training dan Validasi\")\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'], label='Train Loss (phase 1)')\nplt.plot(history.history['val_loss'], label='Val Loss (phase 1)')\nplt.plot(history_fine.history['loss'], label='Train Loss (phase 2)')\nplt.plot(history_fine.history['val_loss'], label='Val Loss (phase 2)')\nplt.title(\"Loss Training dan Validasi\")\nplt.legend()\nplt.show()\n\n# ============================\n# Fungsi Grad-CAM\n# ============================\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        # untuk klasifikasi biner, kelas positif pada indeks 0\n        class_channel = predictions[:, 0]\n    # Gradien output kelas terhadap peta fitur terakhir\n    grads = tape.gradient(class_channel, conv_outputs)\n    # rata-rata gradien untuk setiap channel peta fitur\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    # kalikan setiap saluran peta fitur dengan bobotnya\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    # normalisasi 0..1\n    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + tf.keras.backend.epsilon())\n    return heatmap.numpy()\n\ndef save_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load citra asli\n    img = load_img(img_path)\n    img = img_to_array(img)\n    # Rescale heatmap ke 0-255 dan terapkan colormap Jet\n    heatmap_uint8 = np.uint8(255 * heatmap)\n    jet_colors = mpl.cm.get_cmap(\"jet\")(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap_uint8]\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = img_to_array(jet_heatmap)\n    # Superimpose heatmap ke citra asli\n    superimposed = jet_heatmap * alpha + img\n    superimposed = tf.keras.preprocessing.image.array_to_img(superimposed)\n    superimposed.save(cam_path)\n\n# ============================\n# Hasilkan Grad-CAM Heatmap pada Beberapa Contoh Gambar Test\n# ============================\nlast_conv_layer = \"top_conv\"  # nama lapisan conv terakhir pada EfficientNetB0\ngradcam_dir = \"gradcam_outputs\"\nos.makedirs(gradcam_dir, exist_ok=True)\n\nfilenames = test_gen.filenames  # daftar file relatif dalam test_dir\nfor i, fname in enumerate(filenames[:5]):  # misalnya 5 gambar pertama\n    img_path = os.path.join(test_dir, fname)\n    img = load_img(img_path, target_size=(224, 224))\n    x = img_to_array(img) / 255.0\n    x = np.expand_dims(x, axis=0)\n    # Hasilkan heatmap Grad-CAM\n    heatmap = make_gradcam_heatmap(x, model, last_conv_layer_name=last_conv_layer)\n    cam_path = os.path.join(gradcam_dir, f\"gradcam_{i}.jpg\")\n    save_gradcam(img_path, heatmap, cam_path=cam_path, alpha=0.4)\n    print(f\"Saved Grad-CAM for {fname} as {cam_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}